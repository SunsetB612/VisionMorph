import cv2
import numpy as np
import torch
import torchvision
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import os
import sys
import pandas as pd
from transformers import CLIPModel, CLIPProcessor, BertTokenizer, BertForSequenceClassification
import ssl
import requests
import base64
import json
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.optim import AdamW
from docx import Document
from pathlib import Path

# åŸºç¡€ç¯å¢ƒé…ç½®
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
ssl._create_default_https_context = ssl._create_unverified_context  # è§£å†³HTTPSè¯ä¹¦é—®é¢˜

MODULE_ROOT = Path(__file__).resolve().parent
DEFAULT_RESOURCE_ROOT = MODULE_ROOT


# ========== 1. è±†åŒ…APIè°ƒç”¨å·¥å…·ç±» ==========
class DoubaoAPIClient:
    def __init__(self, api_key, model="doubao-seed-1-6-251015"):
        self.api_key = api_key
        self.model = model

        self.api_url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
        self.session = requests.Session()
        self.session.mount("https://", requests.adapters.HTTPAdapter(max_retries=3))

    def image_to_base64(self, image_np):
        """å°†numpyå›¾åƒè½¬ä¸ºbase64"""
        try:
            success, img_encoded = cv2.imencode(".jpg", image_np, [int(cv2.IMWRITE_JPEG_QUALITY), 85])
            if not success:
                return "å›¾ç‰‡ç¼–ç å¤±è´¥"
            return base64.b64encode(img_encoded).decode("utf-8")
        except Exception as e:
            return f"å›¾ç‰‡ç¼–ç å¤±è´¥: {str(e)}"

    def get_shooting_suggestions(self, original_img, crop_img, region_type, coords, direction_desc):
        """è°ƒç”¨è±†åŒ…APIç”Ÿæˆæ‹æ‘„å»ºè®®"""
        # ç¼–ç å›¾ç‰‡
        original_base64 = self.image_to_base64(original_img)
        crop_base64 = self.image_to_base64(crop_img)
        
        if "å¤±è´¥" in original_base64 or "å¤±è´¥" in crop_base64:
            return f"å›¾ç‰‡å¤„ç†é”™è¯¯ï¼šåŸå§‹å›¾({original_base64}) | è£å‰ªå›¾({crop_base64})"

        # ä¼˜åŒ–æç¤ºè¯
        prompt = f"""
        ä½ æ˜¯ä¸“ä¸šæ‘„å½±æ„å›¾æŒ‡å¯¼ä¸“å®¶ï¼Œéœ€ç”Ÿæˆè¯¦ç»†ã€è§„èŒƒä¸”æ˜“æ‡‚çš„æ‹æ‘„å»ºè®®ï¼Œè¯­è¨€é€‚åº¦ä¹¦é¢åŒ–ï¼Œé¿å…è¿‡äºå£è¯­åŒ–æˆ–éšæ„è¡¨è¿°ï¼Œæ€»é•¿åº¦ä¸å°‘äº500å­—ï¼Œä¸”ä»…è¾“å‡ºçº¯æ–‡å­—å†…å®¹ï¼š
        1. æ ¼å¼è¦æ±‚ï¼šåˆ é™¤æ‰€æœ‰Markdownæ ¼å¼å…ƒç´ ï¼ˆåŒ…æ‹¬æ˜Ÿå·ã€äº•å·ã€åˆ—è¡¨ç¬¦å·ã€æ‹¬å·åµŒå¥—ç­‰ï¼‰ï¼Œä»…ç”¨æ®µè½å¼çº¯æ–‡å­—å‘ˆç°ï¼Œæ®µè½é—´å¯é€‚å½“æ¢è¡ŒåŒºåˆ†é€»è¾‘ï¼›
        2. åˆ†æè£å‰ªå›¾ä¼˜ç‚¹ï¼šç»“åˆç”»é¢å…ƒç´ ã€æ„å›¾é€»è¾‘ã€è‰²å½©æ­é…ç­‰ç»´åº¦ï¼Œå…·ä½“è¯´æ˜ä¼˜åŠ¿æ‰€åœ¨ï¼Œå¦‚ä¸»ä½“å®šä½ç²¾å‡†ã€ç”»é¢å¹³è¡¡æ„Ÿè‰¯å¥½ã€è‰²å½©è¿‡æ¸¡è‡ªç„¶ç­‰ï¼Œé¿å…ç¬¼ç»Ÿè¡¨è¿°ï¼›
        3. æŒ‡å‡ºè£å‰ªå›¾ä¸è¶³ï¼šå®¢è§‚è¯´æ˜å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ä¸»ä½“è¾¹ç¼˜ç•™ç™½ä¸è¶³ã€ç”»é¢æ¯”ä¾‹ä¸å¤Ÿåè°ƒã€èƒŒæ™¯å…ƒç´ ç•¥æ˜¾æ‚ä¹±ç­‰ï¼Œè¡¨è¿°ä¸­è‚¯ä¸”ä¸ç”Ÿç¡¬ï¼›
        4. ç»™å‡ºå…·ä½“ä¼˜åŒ–å»ºè®®ï¼šç»“åˆåŸå§‹å…¨æ™¯å›¾å’Œæ–¹ä½è¯´æ˜ï¼Œä»æ‹æ‘„è§’åº¦è°ƒæ•´ã€ç„¦è·é€‰æ‹©ã€ä¸»ä½“ä½ç½®æ‘†æ”¾ã€èƒŒæ™¯å–èˆç­‰æ–¹é¢å±•å¼€ï¼Œæ¯æ¡å»ºè®®éœ€åŒ…å«æ“ä½œæ–¹æ³•å’Œä¼˜åŒ–åŸç†ï¼Œç¡®ä¿å¯è½åœ°ï¼›
        5. è¡¥å……å»¶ä¼¸æŠ€å·§ï¼šå¢åŠ 2-3æ¡ä¸åœºæ™¯é€‚é…çš„æ‘„å½±æŠ€å·§ï¼Œå¦‚å…‰çº¿åˆ©ç”¨ã€å±‚æ¬¡æ„Ÿè¥é€ ã€ç”»é¢å…ƒç´ å–èˆç­‰ï¼Œä¸°å¯Œå»ºè®®çš„å®ç”¨æ€§ï¼›
        6. æ•´ä½“è¦æ±‚ï¼šç»“æ„æ¸…æ™°ã€è¯­è¨€æµç•…è§„èŒƒï¼Œå…¼é¡¾ä¸“ä¸šæ€§å’Œå¯è¯»æ€§ï¼Œé€‚é…æ‘„å½±çˆ±å¥½è€…ç†è§£å’Œæ“ä½œï¼Œæ— ä»»ä½•æ ¼å¼ç¬¦å·å¹²æ‰°é˜…è¯»ã€‚
    
        è¡¥å……ä¿¡æ¯ï¼š
        - è£å‰ªåŒºåŸŸç±»å‹ï¼š{region_type}
        - è£å‰ªåæ ‡ï¼ˆå·¦ä¸Šè§’x,yï¼›å³ä¸‹è§’x2,y2ï¼‰ï¼š{coords}
        - æ–¹ä½è¯´æ˜ï¼š{direction_desc}
        """

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url", 
                        "image_url": f"data:image/jpeg;base64,{original_base64}"
                    },
                    {
                        "type": "image_url", 
                        "image_url": f"data:image/jpeg;base64,{crop_base64}"
                    },
                    {
                        "type": "text", 
                        "text": prompt
                    }
                ]
            }
        ]

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        data = {
            "model": self.model,
            "max_tokens": 2000,
            "messages": messages,
            "stream": False
        }

        try:
            response = self.session.post(
                self.api_url, 
                headers=headers, 
                json=data,
                timeout=3000
            )
            
            # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            print(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"APIå“åº”å†…å®¹: {response.text[:500]}...")
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"].strip()
            else:
                return f"APIè¿”å›æ ¼å¼å¼‚å¸¸: {json.dumps(result, ensure_ascii=False)[:200]}"
                
        except requests.exceptions.HTTPError as http_err:
            error_detail = f"HTTPé”™è¯¯ {response.status_code}: "
            try:
                error_json = response.json()
                error_detail += error_json.get("error", {}).get("message", str(http_err))
            except:
                error_detail += response.text[:200]
            return error_detail
            
        except requests.exceptions.ConnectionError:
            return "ç½‘ç»œè¿æ¥å¤±è´¥ï¼šè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒDNSé…ç½®"
        except Exception as e:
            return f"APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}"


# ========== 2. è¯»å–Wordæ–‡æ¡£æ–¹ä½è¯´æ˜ï¼ˆä¿æŒå…¼å®¹ï¼‰ ==========
def read_direction_desc(docx_path):
    """è¯»å–docxä¸­çš„æ–¹ä½è¯´æ˜æ–‡æœ¬"""
    try:
        if not os.path.exists(docx_path):
            return f"æ–¹ä½è¯´æ˜æ–‡æ¡£ä¸å­˜åœ¨ï¼š{docx_path}"
        doc = Document(docx_path)
        # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬ï¼ˆå¿½ç•¥ç©ºæ®µè½ï¼‰
        direction_desc = "\n".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])
        return direction_desc if direction_desc else "æ–‡æ¡£æ— æœ‰æ•ˆæ–¹ä½è¯´æ˜"
    except Exception as e:
        return f"æ–‡æ¡£è¯»å–å¤±è´¥ï¼š{str(e)}ï¼ˆå»ºè®®æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸ºæ ‡å‡†docxæ ¼å¼ï¼‰"


# ========== 3. å…¨æ™¯å›¾è£å‰ªåŠŸèƒ½ ==========
class PanoramaCropper:
    def __init__(self, yolo_path=None):
        self.srm_model = self._load_srm_model()
        self.yolo_model = self._load_yolo_model(yolo_path)
        self.segmentation_model = self._load_segmentation_model()
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        self.priority_classes = {'person': 0, 'cat': 15, 'dog': 16, 'car': 2, 'bicycle': 1, 
                                 'bird': 14, 'flower': 58, 'tree': 59, 'building': 66}
    
    def _load_srm_model(self):
        class SimpleSRM(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, padding=1)
                self.conv2 = torch.nn.Conv2d(64, 32, kernel_size=3, padding=1)
                self.conv3 = torch.nn.Conv2d(32, 1, kernel_size=3, padding=1)
                self.relu = torch.nn.ReLU()
            def forward(self, x):
                x = self.relu(self.conv1(x))
                x = self.relu(self.conv2(x))
                x = torch.sigmoid(self.conv3(x))
                return x
        model = SimpleSRM()
        model.eval()
        return model
    
    def _load_yolo_model(self, yolo_path):
        if yolo_path is None:
            print("æ­£åœ¨åŠ è½½YOLOv5æ¨¡å‹ï¼ˆåœ¨çº¿ä¸‹è½½ï¼‰...")
            model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
        else:
            if not os.path.exists(yolo_path):
                raise FileNotFoundError(f"YOLOv5è·¯å¾„ä¸å­˜åœ¨: {yolo_path}")
            print(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½YOLOv5æ¨¡å‹: {yolo_path}")
            sys.path.insert(0, yolo_path)
            try:
                model = torch.hub.load(yolo_path, 'yolov5s', source='local', pretrained=True)
            except Exception as e:
                print(f"æœ¬åœ°åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½...")
                model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
        model.eval()
        return model
    
    def _load_segmentation_model(self):
        model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)
        model.eval()
        return model
    
    def frequency_tuned_saliency(self, image):
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        l_mean, a_mean, b_mean = np.mean(l), np.mean(a), np.mean(b)
        saliency = np.sqrt(np.square(l - l_mean) + np.square(a - a_mean) + np.square(b - b_mean))
        saliency = (saliency - np.min(saliency)) / (np.max(saliency) - np.min(saliency)) * 255
        return saliency.astype(np.uint8)
    
    def srm_saliency_detection(self, image):
        img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        img_tensor = self.transform(img).unsqueeze(0)
        with torch.no_grad():
            output = self.srm_model(img_tensor)
        saliency_map = output.squeeze().numpy()
        saliency_map = (saliency_map * 255).astype(np.uint8)
        saliency_map = cv2.resize(saliency_map, (image.shape[1], image.shape[0]))
        return saliency_map
    
    def object_detection(self, image):
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.yolo_model(rgb_image)
        return results.pandas().xyxy[0]
    
    def image_segmentation(self, image):
        img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        img_tensor = self.transform(img).unsqueeze(0)
        with torch.no_grad():
            output = self.segmentation_model(img_tensor)['out']
        pred = torch.argmax(output, dim=1).squeeze().numpy()
        pred = cv2.resize(pred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)
        return pred
    
    def get_potential_regions(self, image, method='frequency_tuned'):
        if method == 'frequency_tuned':
            saliency_map = self.frequency_tuned_saliency(image)
        elif method == 'srm':
            saliency_map = self.srm_saliency_detection(image)
        else:
            saliency_map = self.frequency_tuned_saliency(image)
        
        potential_regions = []
        _, binary_saliency = cv2.threshold(saliency_map, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        contours, _ = cv2.findContours(binary_saliency, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            if cv2.contourArea(contour) > 800:
                x, y, w, h = cv2.boundingRect(contour)
                potential_regions.append((x, y, w, h, 'saliency'))
        
        try:
            detections = self.object_detection(image)
            if hasattr(detections, 'iterrows'):
                for _, row in detections.iterrows():
                    confidence = row.get('confidence', 0)
                    class_name = row.get('name', '')
                    x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
                    w, h = x2 - x1, y2 - y1
                    if (class_name in self.priority_classes and confidence > 0.4) or (confidence > 0.6):
                        if w * h > 400:
                            potential_regions.append((x1, y1, w, h, f'object_{class_name}'))
        except Exception as e:
            print(f"ç›®æ ‡æ£€æµ‹å¤±è´¥: {e}")
        
        try:
            segmentation = self.image_segmentation(image)
            unique_classes, counts = np.unique(segmentation, return_counts=True)
            top_classes = unique_classes[np.argsort(counts)[-3:]]
            for cls in top_classes:
                if cls == 0:
                    continue
                mask = (segmentation == cls).astype(np.uint8) * 255
                contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                for contour in contours:
                    if cv2.contourArea(contour) > 1200:
                        x, y, w, h = cv2.boundingRect(contour)
                        potential_regions.append((x, y, w, h, 'segmentation'))
        except Exception as e:
            print(f"å›¾åƒåˆ†å‰²å¤±è´¥: {e}")
        
        return potential_regions, saliency_map, segmentation
    
    def crop_regions(self, image, regions, top_n=5):
        if not regions:
            h, w = image.shape[:2]
            regions = [
                (0, 0, w//2, h//2, 'default'),
                (w//2, 0, w//2, h//2, 'default'),
                (0, h//2, w//2, h//2, 'default'),
                (w//2, h//2, w//2, h//2, 'default'),
                (w//4, h//4, w//2, h//2, 'default_center')
            ]
        
        unique_regions = []
        seen = set()
        for x, y, w, h, rtype in regions:
            key = (round(x/50), round(y/50), round(w/50), round(h/50))
            if key not in seen:
                seen.add(key)
                unique_regions.append((x, y, w, h, rtype))
        
        def region_priority(region):
            rtype = region[4]
            area = region[2] * region[3]
            if rtype.startswith('object'):
                return (10, area)
            elif rtype == 'segmentation':
                return (5, area)
            else:
                return (1, area)
        
        unique_regions.sort(key=region_priority, reverse=True)
        cropped_images = []
        
        for i, (x, y, w, h, rtype) in enumerate(unique_regions[:top_n]):
            x = max(0, x)
            y = max(0, y)
            x2 = min(image.shape[1], x + w)
            y2 = min(image.shape[0], y + h)
            if x2 > x and y2 > y:
                cropped = image[y:y2, x:x2]
                cropped_images.append((cropped, rtype, (x, y, x2, y2)))
        
        while len(cropped_images) < top_n:
            h, w = image.shape[:2]
            default_regions = [
                (w//6, h//6, 2*w//3, 2*h//3, 'default_supplement'),
                (0, h//3, w//2, 2*h//3, 'default_supplement'),
                (w//2, h//3, w//2, 2*h//3, 'default_supplement')
            ]
            for reg in default_regions:
                if len(cropped_images) >= top_n:
                    break
                x, y, w_reg, h_reg, rtype = reg
                x2 = x + w_reg
                y2 = y + h_reg
                cropped = image[y:y2, x:x2]
                cropped_images.append((cropped, rtype, (x, y, x2, y2)))
        
        return cropped_images
    
    def process_panorama(self, image_path, top_n=5):
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}ï¼ˆè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼‰")
        regions, saliency_map, segmentation = self.get_potential_regions(image)
        cropped_regions = self.crop_regions(image, regions, top_n)
        return {
            'original': image,
            'saliency_map': saliency_map,
            'segmentation': segmentation,
            'cropped_regions': cropped_regions
        }


# ========== 4. æ„å›¾åˆ†æåŠŸèƒ½ ==========
class MultimodalCompositionAnalyzer:
    def __init__(self, bert_dir=None):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.resource_root = DEFAULT_RESOURCE_ROOT
        self.local_clip_dir = str(self.resource_root / "clip-vit-base-patch32")
        if not os.path.exists(self.local_clip_dir):
            raise FileNotFoundError(f"CLIPæ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼š{self.local_clip_dir}")
        try:
            print(f"æ­£åœ¨åŠ è½½æœ¬åœ°CLIPæ¨¡å‹ï¼š{self.local_clip_dir}")
            self.clip_model = CLIPModel.from_pretrained(self.local_clip_dir).to(self.device)
            self.clip_processor = CLIPProcessor.from_pretrained(self.local_clip_dir)
        except Exception as e:
            raise RuntimeError(f"åŠ è½½CLIPæ¨¡å‹å¤±è´¥ï¼š{str(e)}")
        
        self.composition_aspects = [
            "è‰¯å¥½çš„æ„å›¾", "ç³Ÿç³•çš„æ„å›¾", "å¹³è¡¡çš„æ„å›¾", "ä¸å¹³è¡¡çš„æ„å›¾",
            "ä¸»ä½“çªå‡º", "ä¸»ä½“ä¸çªå‡º", "é€‚å½“çš„ç•™ç™½", "è¿‡å¤šçš„ç•™ç™½",
            "è‰¯å¥½çš„å±‚æ¬¡æ„Ÿ", "ç¼ºä¹å±‚æ¬¡æ„Ÿ", "æ¸…æ™°çš„è§†è§‰å¼•å¯¼", "æ··ä¹±çš„è§†è§‰å¼•å¯¼",
            "é»„é‡‘æ¯”ä¾‹æ„å›¾", "ä¸ç¬¦åˆé»„é‡‘æ¯”ä¾‹", "å¯¹ç§°æ„å›¾", "ä¸å¯¹ç§°æ„å›¾",
            "å‰æ™¯è™šåŒ–", "æ— å‰æ™¯", "èƒŒæ™¯ç®€æ´", "èƒŒæ™¯æ‚ä¹±",
            "è‰²å½©åè°ƒ", "è‰²å½©å†²çª", "æ˜æš—å¯¹æ¯”é€‚ä¸­", "è¿‡äº®", "è¿‡æš—"
        ]
        
        self.local_bert_dir = (
            bert_dir if bert_dir else str(self.resource_root / "bert-base-chinese")
        )
        try:
            print(f"æ­£åœ¨åŠ è½½BERTæ¨¡å‹ï¼š{self.local_bert_dir}")
            self.bert_tokenizer = BertTokenizer.from_pretrained(self.local_bert_dir, local_files_only=True)
            self.bert_model = BertForSequenceClassification.from_pretrained(
                self.local_bert_dir, num_labels=1, local_files_only=True
            ).to(self.device)
        except Exception as e:
            raise RuntimeError(f"BERTæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        
        self.style_labels = [
            "ç®€çº¦", "å¤æ‚", "å¯¹ç§°", "ä¸å¯¹ç§°", "åŠ¨æ€", "é™æ€",
            "ç´§å‡‘", "æ¾æ•£", "å¹³è¡¡", "å¯¹æ¯”å¼ºçƒˆ", "æŸ”å’Œ"
        ]
    
    def load_image(self, image_np):
        image = Image.fromarray(cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB))
        inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
        return inputs["pixel_values"]
    
    def frequency_tuned_saliency(self, image):
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        l_mean, a_mean, b_mean = np.mean(l), np.mean(a), np.mean(b)
        saliency = np.sqrt(np.square(l - l_mean) + np.square(a - a_mean) + np.square(b - b_mean))
        saliency = (saliency - np.min(saliency)) / (np.max(saliency) - np.min(saliency)) * 255
        return saliency.astype(np.uint8)
    
    def get_image_caption(self, image):
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(pixel_values=image)
            text_templates = [
                "a photo with {}", 
                "an image with {}", 
                "a picture with {}", 
                "a shot with {}"
            ]
            text_list = []
            for comp_aspect in self.composition_aspects:
                for template in text_templates:
                    text_list.append(template.format(comp_aspect))
            
            text_inputs = self.clip_processor(
                text=text_list,
                return_tensors="pt",
                padding=True,
                truncation=True
            ).to(self.device)
            text_features = self.clip_model.get_text_features(**text_inputs)
            
            image_features /= image_features.norm(dim=-1, keepdim=True)
            text_features /= text_features.norm(dim=-1, keepdim=True)
            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            values, indices = similarity[0].topk(5)
            
            caption_parts = []
            seen_captions = set()
            for idx in indices:
                orig_idx = idx // len(text_templates)
                caption = self.composition_aspects[orig_idx]
                if caption not in seen_captions:
                    seen_captions.add(caption)
                    caption_parts.append(caption)
                if len(caption_parts) >= 5:
                    break
            return ", ".join(caption_parts)
    
    def predict_style(self, image):
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(pixel_values=image)
            text_list = [f"a photo with {s} style" for s in self.style_labels]
            text_inputs = self.clip_processor(text=text_list, return_tensors="pt", padding=True, truncation=True).to(self.device)
            text_features = self.clip_model.get_text_features(**text_inputs)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            text_features /= text_features.norm(dim=-1, keepdim=True)
            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            values, indices = similarity[0].topk(3)
            styles = [(self.style_labels[idx], float(values[i])) for i, idx in enumerate(indices)]
            return styles
    
    def get_composition_score(self, caption, image_crop):
        inputs = self.bert_tokenizer(
            caption, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=512
        ).to(self.device)
        with torch.no_grad():
            outputs = self.bert_model(**inputs)
            score = outputs.logits.item()
        
        h, w = image_crop.shape[:2]
        saliency_map = self.frequency_tuned_saliency(image_crop)
        _, binary = cv2.threshold(saliency_map, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        subject_ratio = np.sum(binary > 0) / (h * w)
        if 0.3 < subject_ratio < 0.7:
            score += 1.2
        elif 0.2 < subject_ratio <= 0.3 or 0.7 <= subject_ratio < 0.8:
            score += 0.5
        elif subject_ratio <= 0.2 or subject_ratio >= 0.8:
            score -= 0.8
        
        aspect_ratio = w / h
        ideal_ratios = [16/9, 4/3, 1.0, 3/4, 9/16]
        min_diff = min(abs(aspect_ratio - r) for r in ideal_ratios)
        score += (0.5 - min_diff) * 3
        
        edge_margin = min(h, w) * 0.1
        edge_mask = np.zeros_like(binary)
        edge_mask[:int(edge_margin), :] = 1
        edge_mask[-int(edge_margin):, :] = 1
        edge_mask[:, :int(edge_margin)] = 1
        edge_mask[:, -int(edge_margin):] = 1
        edge_subject_ratio = np.sum((binary > 0) & (edge_mask == 1)) / (np.sum(binary > 0) + 1e-6)
        if edge_subject_ratio > 0.3:
            score -= 0.6
        
        score = np.clip(score, 0, 10)
        return round(score, 1)
    
    def generate_suggestions(self, caption, styles):
        suggestions = []
        if "ä¸»ä½“ä¸çªå‡º" in caption:
            suggestions.append("å»ºè®®çªå‡ºä¸»ä½“ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´ç„¦è·æˆ–æ„å›¾æ¥å®ç°")
        if "ä¸å¹³è¡¡çš„æ„å›¾" in caption:
            suggestions.append("æ„å›¾ç•¥æ˜¾ä¸å¹³è¡¡ï¼Œå°è¯•å°†ä¸»ä½“æ”¾åœ¨é»„é‡‘åˆ†å‰²ç‚¹ä½ç½®")
        if "è¿‡å¤šçš„ç•™ç™½" in caption:
            suggestions.append("ç•™ç™½è¿‡å¤šï¼Œå¯ä»¥è€ƒè™‘ç¼©å°å–æ™¯èŒƒå›´")
        if "ç¼ºä¹å±‚æ¬¡æ„Ÿ" in caption:
            suggestions.append("ç¼ºä¹å±‚æ¬¡æ„Ÿï¼Œå°è¯•è°ƒæ•´æ‹æ‘„è§’åº¦æˆ–å¢åŠ å‰æ™¯å…ƒç´ ")
        if "èƒŒæ™¯æ‚ä¹±" in caption:
            suggestions.append("èƒŒæ™¯è¾ƒä¸ºæ‚ä¹±ï¼Œå»ºè®®æ›´æ¢ç®€æ´èƒŒæ™¯æˆ–ä½¿ç”¨å¤§å…‰åœˆè™šåŒ–èƒŒæ™¯")
        if "è‰²å½©å†²çª" in caption:
            suggestions.append("è‰²å½©æ­é…å­˜åœ¨å†²çªï¼Œå¯è°ƒæ•´ç™½å¹³è¡¡æˆ–åæœŸè°ƒè‰²ç»Ÿä¸€è‰²è°ƒ")
        if not suggestions:
            suggestions.append("æ„å›¾è‰¯å¥½ï¼Œä¿æŒå½“å‰é£æ ¼å³å¯")
        position_suggestions = [
            "å‘å·¦å¹³ç§»çº¦ä¸€ç±³å¯ä»¥è·å¾—æ›´å¥½çš„èƒŒæ™¯",
            "å‘å³å¾®è°ƒå¯ä»¥å¹³è¡¡ç”»é¢å…ƒç´ ",
            "ç¨å¾®é™ä½æ‹æ‘„é«˜åº¦å¯ä»¥å¢å¼ºä¸»ä½“è¡¨ç°åŠ›",
            "æé«˜æ‹æ‘„è§’åº¦å¯ä»¥å±•ç°æ›´å¤šç¯å¢ƒä¿¡æ¯"
        ]
        suggestions.append(np.random.choice(position_suggestions))
        return suggestions
    
    def analyze(self, image_np):
        try:
            image = self.load_image(image_np)
            caption = self.get_image_caption(image)
            styles = self.predict_style(image)
            score = self.get_composition_score(caption, image_np)
            suggestions = self.generate_suggestions(caption, styles)
            return {
                "composition_score": score,
                "style_labels": [f"{s[0]}({s[1]:.2f})" for s in styles],
                "explanation": f"å›¾åƒåˆ†ææ˜¾ç¤º: {caption}",
                "suggestions": suggestions
            }
        except Exception as e:
            return {"error": f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}"}


# ========== 5. BERTå¾®è°ƒä¼˜åŒ– ==========
class CompositionDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        encoding = self.tokenizer(
            item["text"], 
            return_tensors="pt", 
            padding="max_length", 
            truncation=True, 
            max_length=self.max_len
        )
        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(item["score"], dtype=torch.float32)
        }

def load_cadb_data(cadb_dir):
    elements_path = os.path.join(cadb_dir, "annotations", "composition_elements.json")
    scores_path = os.path.join(cadb_dir, "annotations", "composition_scores.json")
    
    if not os.path.exists(elements_path) or not os.path.exists(scores_path):
        raise FileNotFoundError("CADBæ ‡æ³¨æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·ç¡®è®¤elementså’Œscoresæ–‡ä»¶æ˜¯å¦å­˜åœ¨")
    
    with open(elements_path, "r", encoding="utf-8") as f:
        elements = json.load(f)
    with open(scores_path, "r", encoding="utf-8") as f:
        scores = json.load(f)
    
    train_data = []
    for img_id in elements:
        if img_id not in scores:
            continue
        composition_tags = elements[img_id]
        text = ", ".join(composition_tags)
        score = scores[img_id]["mean"] * 2
        train_data.append({"text": text, "score": score})
    
    print(f"æˆåŠŸè§£æCADBæ•°æ®ï¼š{len(train_data)}æ¡æ ·æœ¬")
    return train_data

def fine_tune_bert(train_data, bert_base_dir, save_dir, epochs=3):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = BertTokenizer.from_pretrained(bert_base_dir, local_files_only=True)
    model = BertForSequenceClassification.from_pretrained(
        bert_base_dir, 
        num_labels=1, 
        problem_type="regression"
    ).to(device)
    
    dataset = CompositionDataset(train_data, tokenizer)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
    optimizer = AdamW(model.parameters(), lr=1e-5)
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            optimizer.zero_grad()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()
            
            loss.backward()
            optimizer.step()
        
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{epochs}ï¼Œè®­ç»ƒæŸå¤±ï¼š{avg_loss:.4f}")
        if avg_loss < 0.3:
            print("æŸå¤±å·²è¶³å¤Ÿä½ï¼Œæå‰åœæ­¢å¾®è°ƒ")
            break
    
    os.makedirs(save_dir, exist_ok=True)
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f"âœ… BERTå¾®è°ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³ï¼š{save_dir}")


# ========== 6. ä¸»é€»è¾‘ï¼šä¸²è”æ‰€æœ‰åŠŸèƒ½ ==========
if __name__ == "__main__":
    # 1. é…ç½®è·¯å¾„ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    resource_root = DEFAULT_RESOURCE_ROOT
    example_dir = resource_root / "example"
    input_image_path = str(example_dir / "11.png")
    input_docx_path = str(example_dir / "11.docx")
    yolo_path = str(resource_root / "yolov5")
    cadb_dir = str(resource_root / "image-composition-assessment-dataset-cadb")
    bert_base_dir = str(resource_root / "bert-base-chinese")
    bert_finetuned_dir = str(resource_root / "bert-finetuned-cadb")
    output_dir = str(resource_root / "output")
    os.makedirs(output_dir, exist_ok=True)
    
    # 2. å…³é”®é…ç½®ï¼šè±†åŒ…APIå¯†é’¥
    DOUBAO_API_KEY = "1956a016-06a4-42b4-b86f-17c45a9fade0"
    doubao_client = DoubaoAPIClient(api_key=DOUBAO_API_KEY)
    
    # 3. è¯»å–æ–¹ä½è¯´æ˜
    print("=== æ­¥éª¤1ï¼šè¯»å–æ–¹ä½è¯´æ˜æ–‡æ¡£ ===")
    direction_desc = read_direction_desc(input_docx_path)
    print(f"æ–¹ä½è¯´æ˜ï¼š{direction_desc}")
    
    # 4. BERTæ¨¡å‹å¾®è°ƒ
    print("\n=== æ­¥éª¤2ï¼šæ£€æŸ¥BERTæ¨¡å‹ ===")
    if not os.path.exists(bert_finetuned_dir) or not os.listdir(bert_finetuned_dir):
        print("æ­£åœ¨åŠ è½½CADBæ•°æ®é›†å¹¶å¾®è°ƒBERT...")
        train_data = load_cadb_data(cadb_dir)
        fine_tune_bert(train_data, bert_base_dir, bert_finetuned_dir, epochs=3)
    else:
        print(f"âœ… å·²æ‰¾åˆ°å¾®è°ƒåBERTæ¨¡å‹ï¼š{bert_finetuned_dir}")
    
    # 5. ç”Ÿæˆå…¨æ™¯å›¾è£å‰ª
    print("\n=== æ­¥éª¤3ï¼šç”Ÿæˆè£å‰ªå›¾ ===")
    try:
        cropper = PanoramaCropper(yolo_path=yolo_path)
        crop_result = cropper.process_panorama(input_image_path, top_n=5)
        cropped_regions = crop_result['cropped_regions']
        original_image = crop_result['original']
        print(f"âœ… ç”Ÿæˆ {len(cropped_regions)} å¼ è£å‰ªå›¾")
    except Exception as e:
        print(f"âŒ è£å‰ªå›¾ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        sys.exit(1)
    
    # 6. æ„å›¾åˆ†æ+è±†åŒ…APIç”Ÿæˆå»ºè®®
    print("\n=== æ­¥éª¤4ï¼šåˆ†æè£å‰ªå›¾å¹¶ç”Ÿæˆæ‹æ‘„å»ºè®® ===")
    analyzer = MultimodalCompositionAnalyzer(bert_finetuned_dir)
    excel_data = {
        "å›¾ç‰‡åå­—": [],
        "æ„å›¾åˆ†æ•°": [],
        "è£å‰ªç±»å‹": [],
        "æ–¹ä½è¯´æ˜": [],
        "è±†åŒ…æ‹æ‘„å»ºè®®": []
    }
    
    for i, (cropped_img, region_type, coords) in enumerate(cropped_regions):
        img_name = f"å›¾ç‰‡{i+1}"
        crop_save_path = os.path.join(output_dir, f"cropped_{i+1}.jpg")
        cv2.imwrite(crop_save_path, cropped_img)
        print(f"\næ­£åœ¨å¤„ç† {img_name}ï¼ˆä¿å­˜è·¯å¾„ï¼š{crop_save_path}ï¼‰")
        
        analysis = analyzer.analyze(cropped_img)
        if "error" in analysis:
            print(f"âŒ {img_name} åˆ†æå¤±è´¥ï¼š{analysis['error']}")
            excel_data["å›¾ç‰‡åå­—"].append(img_name)
            excel_data["æ„å›¾åˆ†æ•°"].append("åˆ†æå¤±è´¥")
            excel_data["è£å‰ªç±»å‹"].append(region_type)
            excel_data["æ–¹ä½è¯´æ˜"].append(direction_desc)
            excel_data["è±†åŒ…æ‹æ‘„å»ºè®®"].append("æ„å›¾åˆ†æå¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå»ºè®®")
            continue
        
        print(f"ğŸ” æ­£åœ¨è°ƒç”¨è±†åŒ…APIç”Ÿæˆå»ºè®®...")
        coords_str = f"({coords[0]},{coords[1]})-({coords[2]},{coords[3]})"
        shooting_suggestions = doubao_client.get_shooting_suggestions(
            original_img=original_image,
            crop_img=cropped_img,
            region_type=region_type,
            coords=coords_str,
            direction_desc=direction_desc
        )
        
        # è®°å½•ç»“æœ
        print(f"âœ… {img_name} å¤„ç†å®Œæˆï¼Œåˆ†æ•°ï¼š{analysis['composition_score']}")
        excel_data["å›¾ç‰‡åå­—"].append(img_name)
        excel_data["æ„å›¾åˆ†æ•°"].append(analysis["composition_score"])
        excel_data["è£å‰ªç±»å‹"].append(region_type)
        excel_data["æ–¹ä½è¯´æ˜"].append(direction_desc)
        excel_data["è±†åŒ…æ‹æ‘„å»ºè®®"].append(shooting_suggestions)
    
    # 7. ç”ŸæˆExcelç»“æœ
    print("\n=== æ­¥éª¤5ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
    df = pd.DataFrame(excel_data)
    excel_save_path = os.path.join(output_dir, "æ„å›¾åˆ†ææŠ¥å‘Š_å«è±†åŒ…å»ºè®®.xlsx")
    df.to_excel(excel_save_path, index=False)
    
    # 8. è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ‰ æ‰€æœ‰æ“ä½œå®Œæˆï¼")
    print(f"ğŸ“ è£å‰ªå›¾ç›®å½•ï¼š{output_dir}")
    print(f"ğŸ“Š åˆ†ææŠ¥å‘Šï¼š{excel_save_path}")
